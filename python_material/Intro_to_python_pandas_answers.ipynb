{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Astroinformatics Summerschool Hobart 2017\n",
    "\n",
    "This lesson is adapted from the [Data Carpentry Ecology lesson](http://www.datacarpentry.org/python-ecology-lesson/)\n",
    "- make sure to open this ipython notebook in the same directory as the data used in this notebook\n",
    "\n",
    "We'll be using an etherpad to share solutions to challenges, ask questions and chat:\n",
    "https://beta.etherpad.org/p/ANITA_Hobart2017\n",
    "- etherpad let's you collaborate simultaniously on the same document\n",
    "- everyone has their own identificating colour\n",
    "- there is also a chat function where you can talk and ask questions if you like\n",
    "\n",
    "## Introduction to Python and data analysis using pandas\n",
    "\n",
    "Python is a high-level, interpreted programming language. This means the code is easy to read for humans and there is no need for us to compile it and in many cases we do not have to think too much about the underlying system fro e.g. memory usage.\n",
    "\n",
    "As a consequence, we can use it in two ways:\n",
    "- Using the interpreter as an \"advanced calculator\" in interactive mode:\n",
    "- Executing programs/scripts saved as a text file, usually with *.py extension:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"Hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%run my_script.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Types of Data\n",
    "\n",
    "How information is stored in a DataFrame or a python object affects what we can do with it and the outputs of calculations as well. There are two main types of data that we're explore in this lesson: numeric and character types.\n",
    "\n",
    "\n",
    "## Numeric Data Types\n",
    "\n",
    "Numeric data types include integers and floats. A **floating point** (known as a\n",
    "float) number has decimal points even if that decimal point value is 0. For\n",
    "example: 1.13, 2.0 1234.345. If we have a column that contains both integers and\n",
    "floating point numbers, Pandas will assign the entire column to the float data\n",
    "type so the decimal points are not lost. In a vector or data fram (we learn about these different types later) the entire object or an entire column will be of the same type.\n",
    "\n",
    "An **integer** will never have a decimal point. Thus 1.13 would be stored as 1.\n",
    "1234.345 is stored as 1234. You will often see the data type `Int64` in python\n",
    "which stands for 64 bit integer. The 64 simply refers to the memory allocated to\n",
    "store data in each cell which effectively relates to how many digits it can\n",
    "store in each \"cell\". Allocating space ahead of time allows computers to\n",
    "optimize storage and processing efficiency.\n",
    "\n",
    "\n",
    "\n",
    "## Character Data Types\n",
    "\n",
    "Strings are values that contain numbers and / or characters. \n",
    "For example, a string might be a word, a sentence, or several sentences. \n",
    "A string can also contain or consist of numbers. For instance, '1234' could be stored as a\n",
    "string. As could '10.23'. However **strings that contain numbers can not be used\n",
    "for mathematical operations**!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "text = \"Data Carpentry\"\n",
    "number = 42\n",
    "pi_value = 3.1415"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've assigned data to variables, namely `text`, `number` and `pi_value`,\n",
    "using the assignment operator `=`. The variable called `text` is a string which\n",
    "means it can contain letters and numbers. We could reassign the variable `text`\n",
    "to an integer too - but be careful reassigning variables as this can get \n",
    "confusing.\n",
    "\n",
    "To print out the value stored in a variable we can simply type the name of the\n",
    "variable into the interpreter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "however, in scripts we must use the `print` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Comments start with #\n",
    "# Next line will print out text\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# We also need the print statement if we want to see more than one variable\n",
    "text\n",
    "number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(text, number, pi_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operators\n",
    "\n",
    "We can perform mathematical calculations in Python using the basic operators\n",
    " `+, -, /, *, %`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "6*7\n",
    "2**16\n",
    "13 % 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** In python 2 if we divide one integer by another, we get an integer! **\n",
    "The result in python 3 is different where we get a float.\n",
    "Remember to convert your integers to floats when you want floating point precision for divisions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "10/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to integer\n",
    "a = 6.6\n",
    "int(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert to float\n",
    "b=5\n",
    "float(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "10/float(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use comparison and logic operators:\n",
    "`<, >, ==, !=, <=, >=` and statements of identity such as\n",
    "`and, or, not`. The data type returned by this is \n",
    "called a _boolean_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "3>4\n",
    "True and False\n",
    "True or False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential types: Lists and Tuples\n",
    "\n",
    "### Lists\n",
    "\n",
    "**Lists** are a common data structure to hold an ordered sequence of\n",
    "elements. Each element can be accessed by an index.  Note that Python\n",
    "indexes start with 0 instead of 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numbers = [1,2,3]\n",
    "numbers[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add elements to the end of a list, we can use the `append` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numbers.append(4)\n",
    "print(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Methods** are a way to interact with an object (a list, for example). We can invoke \n",
    "a method using the dot `.` followed by the method name and a list of arguments in parentheses. \n",
    "To find out what methods are available for an object, we can use the built-in `help` command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "help(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also access a list of methods using `dir`. Some methods names are\n",
    "surrounded by double underscores. Those methods are called \"special\", and\n",
    "usually we access them in a different way. For example `__add__` method is\n",
    "responsible for the `+` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dir(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuples\n",
    "\n",
    "A tuple is similar to a list in that it's an ordered sequence of elements. However,\n",
    "tuples can not be changed once created (they are \"immutable\"). Tuples are\n",
    "created by placing comma-separated values inside parentheses `()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a_tuple = (1,2,3)\n",
    "another_tuple = (\"blue\", \"green\", \"red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "1. What happens when you type `a_tuple[2]=5` vs `a_list[1]=5` ?\n",
    "2. Type `type(a_tuple)` into python - what is the object type?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_list=[1,2,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_list[1]=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dictionaries\n",
    "\n",
    "A **dictionary** is a container that holds pairs of objects - keys and values.\n",
    "\n",
    "Dictionaries work a lot like lists - except that you index them with *keys*. \n",
    "You can think about a key as a name for or a unique identifier for a set of values\n",
    "in the dictionary. Keys can only have particular types - they have to be \n",
    "\"hashable\". Strings and numeric types are acceptable, but lists aren't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "translation = {\"one\":1, \"two\":2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "translation[\"one\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev = {1:\"one\", 2:[\"two\", \"birds\"]}\n",
    "rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "bad = {[1,2,3]:3}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add an item to the dictionary we assign a value to a new key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rev[3]=\"three\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge\n",
    "\n",
    "Can you do reassignment in a dictionary? Give it a try. \n",
    "\n",
    "1. First check what `rev` is right now (remember `rev` is the name of our dictionary). \n",
    "    \n",
    "2. Try to reassign the second value (in the *key value pair*) so that it no longer reads \"two\" but instead reads \"apple-sauce\". \n",
    "\n",
    "3. Now display `rev` again to see if it has changed. \n",
    "\n",
    "It is important to note that dictionaries are \"unordered\" and do not remember the\n",
    "sequence of their items (i.e. the order in which key:value pairs were added to \n",
    "the dictionary). Because of this, the order in which items are returned from loops\n",
    "over dictionaries might appear random and can even change with time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev[2]=\"apple-sauce\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working With Pandas DataFrames in Python\n",
    "\n",
    "## Starting in the same spot\n",
    "\n",
    "To help the lesson run smoothly, let's ensure everyone is in the same directory.\n",
    "This should help us avoid path and file name issues. At this time please\n",
    "navigate to the workshop directory. If you working in IPython Notebook be sure\n",
    "that you start your notebook in the workshop directory.\n",
    "\n",
    "A quick aside that there are Python libraries like [OS\n",
    "Library](https://docs.python.org/3/library/os.html) that can work with our\n",
    "directory structure, however, that is not our focus today.\n",
    "\n",
    "If you need to change your directory ```import os``` and use ```os.chdir```\n",
    "\n",
    "## Our Data \n",
    "\n",
    "For this lesson, we will be using the Portal Teaching data, a subset of the data\n",
    "from Ernst et al\n",
    "[Long-term monitoring and experimental manipulation of a Chihuahuan Desert ecosystem near Portal, Arizona, USA](http://www.esapubs.org/archive/ecol/E090/118/default.htm)\n",
    "\n",
    "We will be using files from the [Portal Project Teaching Database](https://figshare.com/articles/Portal_Project_Teaching_Database/1314459).\n",
    "This section will use the `surveys.csv` file which can be found in /data/python/python_data\n",
    "\n",
    "We are studying the species and weight of animals caught in plots in our study\n",
    "area. The dataset is stored as a `.csv` file: each row holds information for a\n",
    "single animal, and the columns represent:\n",
    "\n",
    "| Column           | Description                        |\n",
    "|------------------|------------------------------------|\n",
    "| record_id        | Unique id for the observation      |\n",
    "| month            | month of observation               |\n",
    "| day              | day of observation                 |\n",
    "| year             | year of observation                |\n",
    "| plot             | ID of a particular plot            |\n",
    "| species          | 2-letter code                      |\n",
    "| sex              | sex of animal (\"M\", \"F\")           |\n",
    "| wgt              | weight of the animal in grams      |\n",
    "\n",
    "\n",
    "The first few rows of our first file look like this:\n",
    "\n",
    "```\n",
    "record_id,month,day,year,plot,species,sex,wgt\n",
    "1,7,16,1977,2,NA,M,\n",
    "2,7,16,1977,3,NA,M,\n",
    "3,7,16,1977,2,DM,F,\n",
    "```\n",
    "\n",
    "## About Libraries\n",
    "\n",
    "A library in Python contains a set of tools (called functions) that perform\n",
    "tasks on our data. Importing a library is like getting a piece of lab equipment\n",
    "out of a storage locker and setting it up on the bench for use in a project.\n",
    "Once a library is set up, it can be used or called to perform many tasks.\n",
    "\n",
    "Python doesn't load all of the libraries available to it by default. We have to\n",
    "add an `import` statement to our code in order to use library functions. To import\n",
    "a library, we use the syntax `import libraryName`. If we want to give the\n",
    "library a nickname to shorten the command, we can add `as nickNameHere`.  An\n",
    "example of importing the pandas library using the common nickname `pd` is below.\n",
    "\n",
    "You only need to load a library once during your session. You can load the library when needed\n",
    "or you can load all necessary libraries at the beginning of your script. \n",
    "This is good practice, especially for the readability of your code\n",
    "\n",
    "## Pandas in Python\n",
    "\n",
    "One of the best options for working with tabular data in Python is to use the\n",
    "[Python Data Analysis Library](http://pandas.pydata.org/) (a.k.a. Pandas). The\n",
    "Pandas library provides data structures, produces high quality plots with\n",
    "[matplotlib](http://matplotlib.org/) and integrates nicely with other libraries\n",
    "that use [NumPy](http://www.numpy.org/) (which is another Python library) arrays.\n",
    "\n",
    "Each time we call a function that's in a library, we use the syntax\n",
    "`LibraryName.FunctionName`. Adding the library name with a `.` before the\n",
    "function name tells Python where to find the function. In the example above, we\n",
    "have imported Pandas as `pd`. This means we don't have to type out `pandas` each\n",
    "time we call a Pandas function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#if you need to change your directory\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.chdir(\"../\") #make sure you enter the correct fille path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir(\"./\")\n",
    "os.chdir(\"data/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#check your version, we need v0.19 or higher\n",
    "pd.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading CSV Data Using Pandas\n",
    "\n",
    "We will begin by locating and reading our survey data which are in CSV format.\n",
    "We can use Pandas' `read_csv` function to pull the file directly into a\n",
    "[DataFrame](http://pandas.pydata.org/pandas-docs/stable/dsintro.html#dataframe).\n",
    "\n",
    "## So What's a DataFrame?\n",
    "\n",
    "A DataFrame is a 2-dimensional data structure that can store data of different\n",
    "types (including characters, integers, floating point values, factors and more)\n",
    "in columns. It is similar to a spreadsheet or an SQL table or the `data.frame` in\n",
    "R. A DataFrame always has an index (0-based). An index refers to the position of \n",
    "an element in the data structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# note that pd.read_csv is used because we imported pandas as pd\n",
    "pd.read_csv(\"surveys.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that there were 33,549 rows parsed. Each row has 9\n",
    "columns. The first column is the index of the DataFrame. The index is used to\n",
    "identify the position of the data, but it is not an actual column of the DataFrame. \n",
    "It looks like  the `read_csv` function in Pandas  read our file properly. However, \n",
    "we haven't saved any data to memory so we can work with it.We need to assign the \n",
    "DataFrame to a variable. Remember that a variable is a name for a value, such as `x`, \n",
    "or  `data`. We can create a new  object with a variable name by assigning a value to it using `=`.\n",
    "\n",
    "Let's call the imported survey data `surveys_df`:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surveys_df = pd.read_csv(\"surveys.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice when you assign the imported DataFrame to a variable, Python does not\n",
    "produce any output on the screen. We can print the value of the `surveys_df`\n",
    "object by typing its name into the Python command prompt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manipulating Our Species Survey Data\n",
    "\n",
    "Now we can start manipulating our data. First, let's check the data type of the\n",
    "data stored in `surveys_df` using the `type` method. The `type` method and\n",
    "`__class__` attribute tell us that `surveys_df` is `<class 'pandas.core.frame.DataFrame'>`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "type(surveys_df)\n",
    "surveys_df.__class__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also enter `surveys_df.dtypes` at our prompt to view the data type for each\n",
    "column in our DataFrame. `int64` represents numeric integer values - `int64` cells\n",
    "can not store decimals. `object` represents strings (letters and numbers). `float64`\n",
    "represents numbers with decimals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pandas and base Python use slightly different names for data types. More on this\n",
    "is in the table below:\n",
    "\n",
    "| Pandas Type | Native Python Type | Description |\n",
    "|-------------|--------------------|-------------|\n",
    "| object | string | The most general dtype. Will be assigned to your column if column has mixed types (numbers and strings). |\n",
    "| int64  | int | Numeric characters. 64 refers to the memory allocated to hold this character. |\n",
    "| float64 | float | Numeric characters with decimals. If a column contains numbers and NaNs(see below), pandas will default to float64, in case your missing value has a decimal. |\n",
    "| datetime64, timedelta[ns] | N/A (but see the [datetime] module in Python's standard library) | Values meant to hold time data. Look into these for time series experiments. |\n",
    "\n",
    "[datetime]: http://doc.python.org/2/library/datetime.html\n",
    "\n",
    "\n",
    "## Remember the way python treats integer division?\n",
    "\n",
    "**In python 2 integer division returns and integer, as opposed to python 3 where we get a float.**\n",
    "If at least one of the numebrs is a float then we get a float.\n",
    "Alternatively we could use ```from __future__ import division```, which will then treat integer division in\n",
    "python 2 as python 3. However, be aware that if the way python handles division is changed in the future your code might break. So choose what you think is the most stable option for your scripts!\n",
    "\n",
    "\n",
    "To modify the format of values within our data frame we can use the ```astype``` function (also remember in pandas the type is `float64`).\n",
    "Don't forget this is a function within pandas so we use it with a `.`, for example to convert \n",
    "the `record_id` field to floating point values we would run:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# convert the record_id field from an integer to a float\n",
    "surveys_df['record_id']=surveys_df['record_id'].astype('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from __future__ import division"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "10/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens if we try to convert weight values to integers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df['wgt'].astype['int64']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that this throws a value error: `ValueError: Cannot convert NA to\n",
    "integer`. If we look at the `weight` column in the surveys data we notice that\n",
    "there are NaN (**N**ot **a** **N**umber) values. *NaN* values are undefined\n",
    "values that cannot be represented mathematically. Pandas, for example, will read\n",
    "an empty cell in a CSV or Excel sheet as a NaN. NaNs have some desirable\n",
    "properties: if we were to average the `weight` column without replacing our NaNs,\n",
    "Python would know to skip over those cells.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df['wgt'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Note: older pandas version do not know how to handle NaN, please update to v0.19_\n",
    "\n",
    "Check your pandas version using `pd.__version__`, if you need to update open a bash shell\n",
    "and type ```conda update pandas```.\n",
    "\n",
    "---\n",
    "\n",
    "## Missing Data Values - NaN\n",
    "\n",
    "Dealing with missing data values is always a challenge. It's sometimes hard to\n",
    "know why values are missing - was it because of a data entry error? Or data that\n",
    "someone was unable to collect? Should the value be 0? We need to know how\n",
    "missing values are represented in the dataset in order to make good decisions.\n",
    "If we're lucky, we have some metadata that will tell us more about how null\n",
    "values were handled.\n",
    "\n",
    "For instance, in some disciplines, like Remote Sensing, missing data values are\n",
    "often defined as -9999. Having a bunch of -9999 values in your data could really\n",
    "alter numeric calculations. Often in spreadsheets, cells are left empty where no\n",
    "data are available. Pandas will, by default, replace those missing values with\n",
    "NaN. However it is good practice to get in the habit of intentionally marking\n",
    "cells that have no data, with a no data value! That way there are no questions\n",
    "in the future when you (or someone else) explores your data.\n",
    "\n",
    "### Where Are the NaN's?\n",
    "\n",
    "Let's explore the NaN values in our data a bit further. \n",
    "First, let's figure out how many rows contain NaN values for weight. \n",
    "We can do this by identifying how many rows have a NULL value (`.isnull`) or by counting the number of rows that have a meaningful value (e.g., wgt>0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(surveys_df[pd.isnull(surveys_df.wgt)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(surveys_df[surveys_df.wgt>0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can replace all NaN values with zeroes using the `.fillna()` method (after\n",
    "making a copy of the data so we don't lose our work).\n",
    "\n",
    "However, NaN and 0 yield different analysis results. The mean value when NaN\n",
    "values are replaced with 0 is different from when NaN values are simply thrown\n",
    "out or ignored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#replace nan with 0\n",
    "df1 = surveys_df.copy()\n",
    "df1['wgt']=df1['wgt'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#check mean, how does it differ from before?\n",
    "df1['wgt'].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can fill NaN values with any value that we chose. The code below fills all\n",
    "NaN values with a mean for all weight values.\n",
    "\n",
    "```python\n",
    " df1['wgt'] = surveys_df['wgt'].fillna(surveys_df['wgt'].mean())\n",
    "```\n",
    "\n",
    "We could also chose to create a subset of our data, only keeping rows that do\n",
    "not contain NaN values, using `.dropna()` method.\n",
    "\n",
    "**The point is to make conscious decisions about how to manage missing data.** \n",
    "This is where we think about how our data will be used and how these values will\n",
    "impact the scientific conclusions made from the data.\n",
    "\n",
    "Python gives us all of the tools that we need to account for these issues. We\n",
    "just need to be cautious about how the decisions that we make impact scientific\n",
    "results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Ways to View DataFrame objects in Python\n",
    "\n",
    "There are multiple methods that can be used to summarize and access the data\n",
    "stored in DataFrames. Let's try out a few. Note that we call the method by using\n",
    "the object name `surveys_df.method`. So `surveys_df.columns` provides an index\n",
    "of all of the column names in our DataFrame.\n",
    "\n",
    "### Challenges\n",
    "\n",
    "Try out the methods below to see what they return.\n",
    "\n",
    "1. `surveys_df.columns`.\n",
    "2. `surveys_df.head()`. Also, what does `surveys_df.head(15)` do?\n",
    "3. `surveys_df.tail()`.\n",
    "4. `surveys_df.shape`. Take note of the output of the shape method. What format does it return the shape of the DataFrame in?\n",
    "\n",
    "HINT: [More on tuples, here](https://docs.python.org/3/tutorial/datastructures.html#tuples-and-sequences).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Statistics From Data In A Pandas DataFrame\n",
    "\n",
    "We've read our data into Python. Next, let's perform some quick summary\n",
    "statistics to learn more about the data that we're working with. We might want\n",
    "to know how many animals were collected in each plot, or how many of each\n",
    "species were caught. We can perform summary stats quickly using groups. But\n",
    "first we need to figure out what we want to group by.\n",
    "\n",
    "Let's begin by exploring our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df.columns.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Data Using Labels (Column Headings)\n",
    "\n",
    "To recap, we use square brackets `[]` to select a subset of an Python object. For example,\n",
    "we can select all of data from a column named `species` from the `surveys_df`\n",
    "DataFrame by name:\n",
    "\n",
    "```python\n",
    "surveys_df['species']\n",
    "# this syntax, calling the column as an attribute, gives you the same output\n",
    "surveys_df.species\n",
    "```\n",
    "\n",
    "We can also create an new object that contains the data within the `species_id`\n",
    "column as follows:\n",
    "\n",
    "```python\n",
    "# create an object named surveys_species that only contains the `species_id` column\n",
    "surveys_species = surveys_df['species']\n",
    "```\n",
    "\n",
    "We can pass a list of column names too, as an index to select columns in that\n",
    "order. This is useful when we need to reorganize our data.\n",
    "\n",
    "**NOTE:** If a column name is not contained in the DataFrame, an exception\n",
    "(error) will be raised.\n",
    "\n",
    "```python\n",
    "# select the species and plot columns from the DataFrame\n",
    "surveys_df[['species', 'plot']]\n",
    "# what happens when you flip the order?\n",
    "surveys_df[['plot', 'species']]\n",
    "#what happens if you ask for a column that doesn't exist?\n",
    "surveys_df['speciess']\n",
    "```\n",
    "\n",
    "Let's get a list of all the species. The `pd.unique` function tells us all of\n",
    "the unique values in the `species_id` column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_species = surveys_df['species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.unique(surveys_df['species'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenges\n",
    "\n",
    "1. Create a list of unique plot ID's found in the surveys data. Call it\n",
    "   `plot_names`. How many unique plots are there in the data? How many unique\n",
    "   species are in the data?\n",
    "\n",
    "2. What is the difference between `len(plot_names)` and `plot_names.nunique()`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plot_names = pd.unique(surveys_df['plot'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(plot_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_names.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df['plot'].nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groups in Pandas\n",
    "\n",
    "We often want to calculate summary statistics grouped by subsets or attributes\n",
    "within fields of our data. For example, we might want to calculate the average\n",
    "weight of all individuals per plot.\n",
    "\n",
    "The Pandas function `describe` will return descriptive stats including: mean,\n",
    "median, max, min, std and count for a particular column in the data. Pandas'\n",
    "`describe` function will only return summary values for columns containing\n",
    "numeric data.\n",
    "We can calculate basic statistics for all records in a single column using the\n",
    "syntax below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df['wgt'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also extract one specific metric if we wish:\n",
    "\n",
    "```python\n",
    "surveys_df['wgt'].min()\n",
    "surveys_df['wgt'].max()\n",
    "surveys_df['wgt'].mean()\n",
    "surveys_df['wgt'].std()\n",
    "surveys_df['wgt'].count()\n",
    "```\n",
    "\n",
    "But if we want to summarize by one or more variables, for example sex, we can\n",
    "use Pandas' `.groupby` method. Once we've created a groupby DataFrame, we\n",
    "can quickly calculate summary statistics by a group of our choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_data = surveys_df.groupby('sex')\n",
    "# summary statistics for all numeric columns by sex\n",
    "sorted_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# provide the mean for each numeric column by sex\n",
    "sorted_data.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `groupby` command is powerful in that it allows us to quickly generate\n",
    "summary stats.\n",
    "\n",
    "## Quickly Creating Summary Counts in Pandas\n",
    "\n",
    "Let's next count the number of samples for each species. We can do this in a few\n",
    "ways, but we'll use `groupby` combined with a `count()` method.\n",
    "\n",
    "\n",
    "```python\n",
    "# count the number of samples by species\n",
    "species_counts = surveys_df.groupby('species')['record_id'].count()\n",
    "```\n",
    "\n",
    "Or, we can also count just the rows that have the species \"DO\":\n",
    "\n",
    "```python\n",
    "surveys_df.groupby('species')['record_id'].count()['DO']\n",
    "```\n",
    "\n",
    "\n",
    "### Challenge\n",
    "\n",
    "1. How many recorded individuals are female `F` and how many male `M`\n",
    "2. What happens when you group by two columns using the following syntax and\n",
    "    then grab mean values:\n",
    "\t- `sorted_data2 = surveys_df.groupby(['plot','sex'])`\n",
    "\t- `sorted_data2.mean()`\n",
    "3. Summarize weight values for each plot in your data. HINT: you can use the\n",
    "   following syntax to only create summary statistics for one column in your data\n",
    "   `by_plot['wgt'].describe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df.groupby('sex')['record_id'].count()[['F','M']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sorted_data2 = surveys_df.groupby(['plot','sex'])\n",
    "sorted_data2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "by_plot = surveys_df.groupby('plot')\n",
    "by_plot['wgt'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you get #3 right? **A Snippet of the Output from challenge 3 looks like:**\n",
    "\n",
    "```\n",
    "\tplot\n",
    "\t1     count    1903.000000\n",
    "\t      mean       51.822911\n",
    "\t      std        38.176670\n",
    "\t      min         4.000000\n",
    "\t      25%        30.000000\n",
    "\t      50%        44.000000\n",
    "\t      75%        53.000000\n",
    "\t      max       231.000000\n",
    "          ...\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Basic Math Functions\n",
    "\n",
    "If we wanted to, we could perform math on an entire column of our data. For\n",
    "example let's multiply all weight values by 2. A more practical use of this might\n",
    "be to normalize the data according to a mean, area, or some other value\n",
    "calculated from our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# multiply all weight values by 2\n",
    "surveys_df['wgt']*2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick & Easy Plotting Data Using Pandas\n",
    "\n",
    "We can plot our summary stats using Pandas, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make sure figures appear inline in Ipython Notebook\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create a quick bar chart\n",
    "species_counts = surveys_df.groupby('species')['record_id'].count()\n",
    "species_counts.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can also look at how many animals were captured in each plot:\n",
    "total_count = surveys_df['record_id'].groupby(surveys_df['plot']).nunique()\n",
    "# let's plot that too\n",
    "total_count.plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge Activities\n",
    "\n",
    "1. Create a plot of average weight across all species per plot.\n",
    "2. Create a plot of total males versus total females for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing & Slicing in Python\n",
    "\n",
    "We often want to work with subsets of a **DataFrame** object. There are\n",
    "different ways to accomplish this including: using labels (ie, column headings - as used previously),\n",
    "numeric ranges or specific x,y index locations.\n",
    "\n",
    "## Extracting Range based Subsets: Slicing\n",
    "\n",
    "**REMINDER**: Python Uses 0-based Indexing\n",
    "\n",
    "Let's remind ourselves that Python uses 0-based\n",
    "indexing. This means that the first element in an object is located at position\n",
    "0. This is different from other tools like R and Matlab that index elements\n",
    "within objects starting at 1.\n",
    "\n",
    "```python\n",
    "# Create a list of numbers:\n",
    "a = [1,2,3,4,5]\n",
    "```\n",
    "\n",
    "[indexing diagram](https://github.com/datacarpentry/python-ecology-lesson/blob/gh-pages/fig/slicing-slicing.svg)\n",
    "\n",
    "[slicing diagram](https://github.com/datacarpentry/python-ecology-lesson/tree/gh-pages/fig/slicing-slicing.svg)\n",
    "\n",
    "## Challenges\n",
    "\n",
    "1. What value does the code below return?\n",
    "        a[0]\n",
    "2. How about this:\n",
    "        a[5]\n",
    "3. Or this?\n",
    "        a[len(a)]\n",
    "4. In the example above, calling `a[5]` returns an error. Why is that?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [1,2,3,4,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    " a[len(a)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing Subsets of Rows in Python\n",
    "\n",
    "Slicing using the `[]` operator selects a set of rows and/or columns from a\n",
    "DataFrame. To slice out a set of rows, you use the following syntax:\n",
    "`data[start:stop]`. When slicing in pandas the start bound is included in the\n",
    "output. The stop bound is one step BEYOND the row you want to select. So if you\n",
    "want to select rows 0, 1 and 2 your code would look like this:\n",
    "\n",
    "```python\n",
    "# select rows 0,1,2 (but not 3)\n",
    "surveys_df[0:3]\n",
    "```\n",
    "\n",
    "The stop bound in Python is different from what you might be used to in\n",
    "languages like Matlab and R.\n",
    "\n",
    "```python\n",
    "# select the first, second and third rows from the surveys variable\n",
    "surveys_df[0:3]\n",
    "# select the first 5 rows (rows 0,1,2,3,4)\n",
    "surveys_df[:5]\n",
    "# select the last element in the list\n",
    "surveys_df[-1:]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df[-1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also reassign values within subsets of our DataFrame. But before we do that, let's make a \n",
    "copy of our DataFrame so as not to modify our original imported data. \n",
    "\n",
    "```python\n",
    "# copy the surveys dataframe so we don't modify the original DataFrame\n",
    "surveys_copy = surveys_df\n",
    "\n",
    "# set the first three rows of data in the DataFrame to 0\n",
    "surveys_copy[0:3] = 0\n",
    "```\n",
    "\n",
    "Next, try the following code: \n",
    "\n",
    "```python\n",
    "surveys_copy.head()\n",
    "surveys_df.head()\n",
    "```\n",
    "What is the difference between the two data frames?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surveys_copy = surveys_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surveys_copy[0:3] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surveys_copy = surveys_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Referencing Objects vs Copying Objects in Python\n",
    "\n",
    "We might have thought that we were creating a fresh copy of the `surveys_df` objects when we \n",
    "used the code `surveys_copy = surveys_df`. However the statement  y = x doesnâ€™t create a copy of our DataFrame. \n",
    "It creates a new variable y that refers to the **same** object x refers to. This means that there is only one object \n",
    "(the DataFrame), and both x and y refer to it. So when we assign the first 3 columns the value of 0 using the \n",
    "`surveys_copy` DataFrame, the `surveys_df` DataFrame is modified too. To create a fresh copy of the `surveys_df`\n",
    "DataFrame we use the syntax y=x.copy(). But before we have to read the surveys_df again because the current version contains the unintentional changes made to the first 3 columns.\n",
    "\n",
    "```python\n",
    "surveys_df = pd.read_csv(\"surveys.csv\")\n",
    "surveys_copy= surveys_df.copy()\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df = pd.read_csv(\"data/surveys.csv\")\n",
    "surveys_copy= surveys_df.copy()\n",
    "surveys_copy[0:3] = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_copy.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "35549"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(surveys_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing Subsets of Rows and Columns in Python\n",
    "\n",
    "We can select specific ranges of our data in both the row and column directions\n",
    "using either label or integer-based indexing.\n",
    "\n",
    "- `loc`: indexing via *labels* (which can be integers)\n",
    "- `iloc`: indexing via *integers*\n",
    "\n",
    "To select a subset of rows AND columns from our DataFrame, we can use the `iloc`\n",
    "method. For example, we can select month, day and year (columns 2, 3 and 4 if we\n",
    "start counting at 1), like this:\n",
    "\n",
    "```python\n",
    "surveys_df.iloc[0:3, 1:4]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# if you haven't done so yet read the csv back in (we had unwanted changes when \n",
    "# we 'copied' the data frame earlier)\n",
    "surveys_df = pd.read_csv(\"surveys.csv\")\n",
    "\n",
    "#try .iloc\n",
    "surveys_df.iloc[0:3 , 1:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that we asked for a slice from 0:3. This yielded 3 rows of data. When you\n",
    "ask for 0:3, you are actually telling python to start at index 0 and select rows\n",
    "0, 1, 2 **up to but not including 3**.\n",
    "\n",
    "\n",
    "Let's next explore some other ways to index and select subsets of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# select all columns for rows of index values 0 and 10\n",
    "surveys_df.loc[[0, 10], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# what does this do?\n",
    "surveys_df.loc[0, ['species', 'plot', 'wgt']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>record_id</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>year</th>\n",
       "      <th>plot</th>\n",
       "      <th>species</th>\n",
       "      <th>sex</th>\n",
       "      <th>wgt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>M</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>16.0</td>\n",
       "      <td>1977.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>DS</td>\n",
       "      <td>F</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35549</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       record_id  month   day    year  plot species  sex  wgt\n",
       "0            1.0    7.0  16.0  1977.0   2.0     NaN    M  NaN\n",
       "10          11.0    7.0  16.0  1977.0   5.0      DS    F  NaN\n",
       "35549        NaN    NaN   NaN     NaN   NaN     NaN  NaN  NaN"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What happens when you type the code below?\n",
    "surveys_df.loc[[0, 10, 35549], :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: Labels must be found in the DataFrame or you will get a `KeyError`. The\n",
    "start bound and the stop bound are **included**.  When using `loc`, integers\n",
    "*can* also be used, but they refer to the **index label** and not the position. Thus\n",
    "when you use `loc`, and select 1:4, you will get a different result than using\n",
    "`iloc` to select rows 1:4.\n",
    "\n",
    "We can also select a specific data value according to the specific row and\n",
    "column location within the data frame using the `iloc` function:\n",
    "`dat.iloc[row,column]`.\n",
    "\n",
    "\n",
    "```python\n",
    "surveys_df.iloc[2,6]\n",
    "```\n",
    "\n",
    "which gives **output**\n",
    "\n",
    "```\n",
    "'F'\n",
    "```\n",
    "\n",
    "Remember that Python indexing begins at 0. So, the index location [2, 6] selects\n",
    "the element that is 3 rows down and 7 columns over in the DataFrame.\n",
    "\n",
    "## Challenge Activities\n",
    "\n",
    "1. What happens when you type:\n",
    "\t- surveys_df[0:3]\n",
    "\t- surveys_df[:5]\n",
    "\t- surveys_df[-1:]\n",
    "\n",
    "2. What happens when you call:\n",
    "    - `dat.iloc[0:4, 1:4]`\n",
    "    - `dat.loc[0:4, 1:4]`\n",
    "    - How are the two commands different?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df.iloc[2,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subsetting Data Using Criteria\n",
    "\n",
    "We can also select a subset of our data using criteria. For example, we can\n",
    "select all rows that have a year value of 2002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df[surveys_df.year == 2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Or we can select all rows that do not contain the year 2002.\n",
    "surveys_df[surveys_df.year != 2002]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#We can define sets of criteria too:\n",
    "surveys_df[(surveys_df.year >= 1980) & (surveys_df.year <= 1985)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Syntax Cheat Sheet\n",
    "\n",
    "Use can use the syntax below when querying data from a DataFrame. Experiment\n",
    "with selecting various subsets of the \"surveys\" data.\n",
    "\n",
    "* Equals: `==`\n",
    "* Not equals: `!=`\n",
    "* Greater than, less than: `>` or `<`\n",
    "* Greater than or equal to `>=`\n",
    "* Less than or equal to `<=`\n",
    "\n",
    "\n",
    "### Challenge Activities\n",
    "\n",
    "1. Select a subset of rows in the `surveys_df` DataFrame that contain data from\n",
    "   the year 1999 and that contain weight values less than or equal to 8. How\n",
    "   many columns did you end up with? What did your neighbor get?\n",
    "2. You can use the `isin` command in python to query a DataFrame based upon a\n",
    "   list of values as follows:\n",
    "   `surveys_df[surveys_df['species'].isin([listGoesHere])]`. Use the `isin` function\n",
    "   to find all plots that contain particular species in\n",
    "   the surveys DataFrame. How many records contain these values?\n",
    "3. Experiment with other queries. Create a query that finds all rows with a weight value > or equal to 0.\n",
    "4. The `~` symbol in Python can be used to return the OPPOSITE of the selection that you specify in python. \n",
    "It is equivalent to **is not in**. Write a query that selects all rows that are NOT equal to 'M' or 'F' in the surveys\n",
    "data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(surveys_df[(surveys_df.year==1999) & (surveys_df.wgt<=8)])\n",
    "#what about our NaN?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df['species'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df[surveys_df['species'].isin(['DM','SO','ZL'])]['plot'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df[surveys_df['wgt']>=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(surveys_df[~surveys_df['sex'].isin(['F','M'])]))\n",
    "surveys_df[~surveys_df['sex'].isin(['F','M'])].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Masks\n",
    "\n",
    "A mask can be useful to locate where a particular subset of values exist or\n",
    "don't exist - for example,  NaN, or \"Not a Number\" values. To understand masks,\n",
    "we also need to understand `BOOLEAN` objects in python.\n",
    "\n",
    "Boolean values include `true` or `false`. So for example\n",
    "\n",
    "```python\n",
    "# set x to 5\n",
    "x = 5\n",
    "# what does the code below return?\n",
    "x > 5\n",
    "# how about this?\n",
    "x == 5\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we ask python what the value of `x > 5` is, we get `False`. This is because x\n",
    "is not greater than 5 it is equal to 5. To create a boolean mask, you first create the\n",
    "True / False criteria (e.g. values > 5 = True). Python will then assess each\n",
    "value in the object to determine whether the value meets the criteria (True) or\n",
    "not (False). Python creates an output object that is the same shape as\n",
    "the original object, but with a True or False value for each index location.\n",
    "\n",
    "Let's try this out. Let's identify all locations in the survey data that have\n",
    "null (missing or NaN) data values. We can use the `isnull` method to do this.\n",
    "Each cell with a null value will be assigned a value of  `True` in the new\n",
    "boolean object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.isnull(surveys_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To select the rows where there are null values,  we can use \n",
    "the mask as an index to subset our data as follows:\n",
    "\n",
    "```python\n",
    "#To select just the rows with NaN values, we can use the .any method\n",
    "surveys_df[pd.isnull(surveys_df).any(axis=1)]\n",
    "```\n",
    "\n",
    "Note that there are many null or NaN values in the `wgt` column of our DataFrame.\n",
    "We will explore different ways of dealing with these in Lesson 03.\n",
    "\n",
    "We can run `isnull` on a particular column too. What does the code below do?\n",
    "\n",
    "```python\n",
    "# what does this do?\n",
    "empty_weights = surveys_df[pd.isnull(surveys_df).any(axis=1)]['wgt']\n",
    "```\n",
    "\n",
    "Let's take a minute to look at the statement above. \n",
    "\n",
    "We are using the Boolean object as an index. \n",
    "We are asking python to select rows that have a `NaN` value\n",
    "for weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "empty_weights = surveys_df[pd.isnull(surveys_df).any(axis=1)]['wgt']\n",
    "empty_weights.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges\n",
    "\n",
    "1. Create a new DataFrame that only contains observations with sex values that\n",
    "   are **not** female or male. Assign each sex value in the new DataFrame to a\n",
    "   new value of 'x'. Determine the number of null values in the subset.\n",
    "2. Create a new DataFrame that contains only observations that are of sex male\n",
    "   or female and where weight values are greater than 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Merging DataFrames\n",
    "\n",
    "\n",
    "In many \"real world\" situations, the data that we want to use come in multiple\n",
    "files. We often need to combine these files into a single DataFrame to analyze\n",
    "the data. The pandas package provides [various methods for combining\n",
    "DataFrames](http://pandas.pydata.org/pandas-docs/stable/merging.html) including\n",
    "`merge` and `concat`.\n",
    "\n",
    "To work through the examples below, we first need to load the species and\n",
    "surveys files into pandas DataFrames. In iPython:\n",
    "\n",
    "``` python\n",
    "import pandas as pd\n",
    "surveys_df = pd.read_csv('surveys.csv',\n",
    "                         keep_default_na=False, na_values=[\"\"])\n",
    "species_df = pd.read_csv('species.csv',\n",
    "                         keep_default_na=False, na_values=[\"\"])\n",
    "surveys_df.dtypes\n",
    "species_df.dtypes\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "surveys_df = pd.read_csv('surveys.csv',\n",
    "                         keep_default_na=False, na_values=[\"\"])\n",
    "species_df = pd.read_csv('species.csv',\n",
    "                         keep_default_na=False, na_values=[\"\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "species_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take note that the `read_csv` method we used can take some additional options which\n",
    "we didn't use previously. Many functions in python have a set of options that\n",
    "can be set by the user if needed. In this case, we have told Pandas to assign\n",
    "empty values in our CSV to NaN `keep_default_na=False, na_values=[\"\"]`.\n",
    "[http://pandas.pydata.org/pandas-docs/dev/generated/pandas.io.parsers.read_csv.html](More\n",
    "about all of the read_csv options here.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Concatinating\n",
    "\n",
    "We can use the `concat` function in Pandas to append either columns or rows from\n",
    "one DataFrame to another.  Let's grab two subsets of our data to see how this\n",
    "works.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# read in first 10 lines of surveys table\n",
    "survey_sub=surveys_df.head(10)\n",
    "survey_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# grab the last 10 rows (minus the last one)\n",
    "survey_sub_last10 = surveys_df[-11:-1]\n",
    "survey_sub_last10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#reset the index values to the second dataframe appends properly\n",
    "# drop=True option avoids adding new index column with old index values\n",
    "survey_sub_last10 = survey_sub_last10.reset_index(drop=True)\n",
    "survey_sub_last10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we concatenate DataFrames, we need to specify the axis. `axis=0` tells\n",
    "Pandas to stack the second DataFrame under the first one. It will automatically\n",
    "detect whether the column names are the same and will stack accordingly.\n",
    "`axis=1` will stack the columns in the second DataFrame to the RIGHT of the\n",
    "first DataFrame. To stack the data vertically, we need to make sure we have the\n",
    "same columns and associated column format in both datasets. When we stack\n",
    "horizonally, we want to make sure what we are doing makes sense (ie the data are\n",
    "related in some way).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# stack the DataFrames on top of each other\n",
    "vertical_stack = pd.concat([survey_sub,survey_sub_last10], axis=0)\n",
    "vertical_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# place the DataFrames side by side\n",
    "horizontal_stack = pd.concat([survey_sub,survey_sub_last10], axis=1)\n",
    "horizontal_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge - Row Index Values and Concat\n",
    "Have a look at the `vertical_stack` dataframe? Notice anything unusual?\n",
    "The row indexes for the two data frames `survey_sub` and `survey_sub_last10`\n",
    "have been repeated. We can reindex the new dataframe using the `reset_index()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vertical_stack = vertical_stack.reset_index()\n",
    "vertical_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Out Data to CSV\n",
    "\n",
    "We can use the `to_csv` command to do export a DataFrame in CSV format. Note that the code\n",
    "below will by default save the data into the current working directory. We can\n",
    "save it to a different folder by adding the foldername and a slash to the file\n",
    "`vertical_stack.to_csv('foldername/out.csv')`.\n",
    "\n",
    "```python\n",
    "# Write DataFrame to CSV \n",
    "vertical_stack.to_csv('out.csv')\n",
    "```\n",
    "\n",
    "Check out your working directory to make sure the CSV wrote out properly, and\n",
    "that you can open it! If you want, try to bring it back into python to make sure\n",
    "it imports properly.\n",
    "\n",
    "```python\t\n",
    "# for kicks read our output back into python and make sure all looks good\n",
    "new_output = pd.read_csv('out.csv', keep_default_na=False, na_values=[\"\"])\n",
    "```\n",
    "\n",
    "###  Challenge (if we have the time)\n",
    "\n",
    "In the data folder, there are two survey data files: `survey2001.csv` and\n",
    "`survey2002.csv`. Read the data into python and combine the files to make one\n",
    "new data frame. Create a plot of average plot weight by year grouped by sex.\n",
    "Export your results as a CSV and make sure it reads back into python properly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vertical_stack.to_csv('out.csv')\n",
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "new_output = pd.read_csv('out.csv', keep_default_na=False, na_values=[\"\"])\n",
    "new_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Joining DataFrames\n",
    "\n",
    "When we concatenated our DataFrames we simply added them to each other -\n",
    "stacking them either vertically or side by side. Another way to combine\n",
    "DataFrames is to use columns in each dataset that contain common values (a\n",
    "common unique id). Combining DataFrames using a common field is called\n",
    "\"joining\". The columns containing the common values are called \"join key(s)\".\n",
    "Joining DataFrames in this way is often useful when one DataFrame is a \"lookup\n",
    "table\" containing additional data that we want to include in the other. \n",
    "\n",
    "NOTE: This process of joining tables is similar to what we do with tables in an\n",
    "SQL database.\n",
    "\n",
    "For example, the `species.csv` file that we've been working with is a lookup\n",
    "table. This table contains the genus, species and taxa code for 55 species. The\n",
    "species code is unique for each line. These species are identified in our survey\n",
    "data as well using the unique species code. Rather than adding 3 more columns\n",
    "for the genus, species and taxa to each of the 35,549 line Survey data table, we\n",
    "can maintain the shorter table with the species information. When we want to\n",
    "access that information, we can create a query that joins the additional columns\n",
    "of information to the Survey data.\n",
    "\n",
    "Storing data in this way has many benefits including:\n",
    "\n",
    "1. It ensures consistency in the spelling of species attributes (genus, species\n",
    "   and taxa) given each species is only entered once. Imagine the possibilities\n",
    "   for spelling errors when entering the genus and species thousands of times!\n",
    "2. It also makes it easy for us to make changes to the species information once\n",
    "   without having to find each instance of it in the larger survey data.\n",
    "3. It optimizes the size of our data. \n",
    "\n",
    "\n",
    "## Joining Two DataFrames \n",
    "\n",
    "To better understand joins, let's grab the first 10 lines of our data as a\n",
    "subset to work with. We'll use the `.head` method to do this. We'll also read\n",
    "in a subset of the species table. \n",
    "\n",
    "```python\n",
    "# read in first 10 lines of surveys table\n",
    "survey_sub = surveys_df.head(10)\n",
    "\n",
    "# import a small subset of the species data designed for this part of the lesson\n",
    "species_sub = pd.read_csv('species_subset.csv', keep_default_na=False, na_values=[\"\"])\n",
    "```\n",
    "\n",
    "In this example, `species_sub` is the lookup table containing genus, species, and\n",
    "taxa names that we want to join with the data in `survey_sub` to produce a new\n",
    "DataFrame that contains all of the columns from both `species_df` *and*\n",
    "`survey_df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "survey_sub = surveys_df.head(10)\n",
    "species_sub = species_df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identifying join keys\n",
    "\n",
    "To identify appropriate join keys we first need to know which field(s) are\n",
    "shared between the files (DataFrames). We might inspect both DataFrames to\n",
    "identify these columns. If we are lucky, both DataFrames will have columns with\n",
    "the same name that also contain the same data. If we are less lucky, we need to\n",
    "identify a (differently-named) column in each DataFrame that contains the same\n",
    "information.\n",
    "\n",
    "Check the column names for survey_sub and species_sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(survey_sub)\n",
    "print(species_sub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our example, the join key is the column containing the two-letter species\n",
    "identifier. \n",
    "Note this is called `species_id` in species_df but `species` in survey_df.\n",
    "\n",
    "Now that we know the fields with the common species ID attributes in each\n",
    "DataFrame, we are almost ready to join our data. However, since there are\n",
    "[different types of joins](http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/), we\n",
    "also need to decide which type of join makes sense for our analysis.\n",
    "\n",
    "## Inner joins\n",
    "\n",
    "The most common type of join is called an _inner join_. An inner join combines\n",
    "two DataFrames based on a join key and returns a new DataFrame that contains\n",
    "**only** those rows that have matching values in *both* of the original\n",
    "DataFrames. \n",
    "\n",
    "Inner joins yield a DataFrame that contains only rows where the value being\n",
    "joins exists in BOTH tables. An example of an inner join, adapted from [this\n",
    "page](http://blog.codinghorror.com/a-visual-explanation-of-sql-joins/) is below:\n",
    "\n",
    "![Inner join -- courtesy of codinghorror.com](http://blog.codinghorror.com/content/images/uploads/2007/10/6a0120a85dcdae970b012877702708970c-pi.png)\n",
    "\n",
    "The pandas function for performing joins is called `merge` and an Inner join is\n",
    "the default option:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "merged_inner = pd.merge(left=survey_sub,right=species_sub, left_on='species', right_on='species_id')\n",
    "# if the column names in  both dataframes were 'species_id', we could skip the `left_on`\n",
    "# and `right_on` arguments and still get the same result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# what's the size of the output data?\n",
    "merged_inner.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_inner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of an inner join of `survey_sub` and `species_sub` is a new DataFrame\n",
    "that contains the combined set of columns from `survey_sub` and `species_sub`. It\n",
    "*only* contains rows that have two-letter species codes that are the same in\n",
    "both the `survey_sub` and `species_sub` DataFrames. In other words, if a row in\n",
    "`survey_sub` has a value of `species` that does *not* appear in the `species_id`\n",
    "column of `species`, it will not be included in the DataFrame returned by an\n",
    "inner join.  Similarly, if a row in `species_sub` has a value of `species_id`\n",
    "that does *not* appear in the `species` column of `survey_sub`, that row will not\n",
    "be included in the DataFrame returned by an inner join.\n",
    "\n",
    "The two DataFrames that we want to join are passed to the `merge` function using\n",
    "the `left` and `right` argument. The `left_on='species'` argument tells `merge`\n",
    "to use the `species` column as the join key from `survey_sub` (the `left`\n",
    "DataFrame). Similarly , the `right_on='species_id'` argument tells `merge` to\n",
    "use the `species_id` column as the join key from `species_sub` (the `right`\n",
    "DataFrame). For inner joins, the order of the `left` and `right` arguments does\n",
    "not matter.\n",
    "\n",
    "The result `merged_inner` DataFrame contains all of the columns from `survey_sub`\n",
    "(record id, month, day, etc.) as well as all the columns from `species_sub`\n",
    "(species_id, genus, species, and taxa).\n",
    "\n",
    "Notice that `merged_inner` has fewer rows than `survey_sub`. This is an\n",
    "indication that there were rows in `surveys_df` with value(s) for `species` that\n",
    "do not exist as value(s) for `species_id` in `species_df`.\n",
    " \n",
    "## Left joins\n",
    "\n",
    "What if we want to add information from `species_sub` to `survey_sub` without\n",
    "losing any of the information from `survey_sub`? In this case, we use a different\n",
    "type of join called a \"left outer join\", or a \"left join\".\n",
    "\n",
    "Like an inner join, a left join uses join keys to combine two DataFrames. Unlike\n",
    "an inner join, a left join will return *all* of the rows from the `left`\n",
    "DataFrame, even those rows whose join key(s) do not have values in the `right`\n",
    "DataFrame.  Rows in the `left` DataFrame that are missing values for the join\n",
    "key(s) in the `right` DataFrame will simply have null (i.e., NaN or None) values\n",
    "for those columns in the resulting joined DataFrame.\n",
    "\n",
    "Note: a left join will still discard rows from the `right` DataFrame that do not\n",
    "have values for the join key(s) in the `left` DataFrame.\n",
    "\n",
    "![Left Join](http://blog.codinghorror.com/content/images/uploads/2007/10/6a0120a85dcdae970b01287770273e970c-pi.png)\n",
    "\n",
    "A left join is performed in pandas by calling the same `merge` function used for\n",
    "inner join, but using the `how='left'` argument:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_left = pd.merge(left=species_sub,right=survey_sub, how='left', \n",
    "                       left_on='species_id', right_on='species')\n",
    "\n",
    "merged_left"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result DataFrame from a left join (`merged_left`) looks very much like the\n",
    "result DataFrame from an inner join (`merged_inner`) in terms of the columns it\n",
    "contains. However, unlike `merged_inner`, `merged_left` contains the **same\n",
    "number of rows** as the original `survey_sub` DataFrame. When we inspect\n",
    "`merged_left`, we find there are rows where the information that should have\n",
    "come from `species_sub` (i.e., `species_id`, `genus`, and `taxa`) is\n",
    "missing (they contain NaN values):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_left[ pd.isnull(merged_left.genus) ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These rows are the ones where the value of `species` from `survey_sub` (in this\n",
    "case, `PF`) does not occur in `species_sub`.\n",
    "\n",
    "\n",
    "## Other join types\n",
    "\n",
    "The pandas `merge` function supports two other join types:\n",
    "\n",
    "* Right (outer) join: Invoked by passing `how='right'` as an argument. Similar\n",
    "  to a left join, except *all* rows from the `right` DataFrame are kept, while\n",
    "  rows from the `left` DataFrame without matching join key(s) values are\n",
    "  discarded.\n",
    "* Full (outer) join: Invoked by passing `how='outer'` as an argument. This join\n",
    "  type returns the all pairwise combinations of rows from both DataFrames; i.e.,\n",
    "  the result DataFrame will `NaN` where data is missing in one of the dataframes. This join type is\n",
    "  very rarely used.\n",
    "\n",
    "### Challenge\n",
    "\n",
    "1) Create a new DataFrame by joining the contents of the `surveys.csv` and\n",
    "`species.csv` tables. Then calculate and plot the distribution of:\n",
    "\n",
    "    a) taxa by plot\n",
    "    b) taxa by sex by plot\n",
    "    \n",
    "    \n",
    "2) In the data folder, there is a plot.csv file that contains information about the type associated with each plot. Use that data to summarize the number of plots by plot type. Note the difference in column names, ie, `plot_id` and `plot`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "merged_outer = pd.merge(left=survey_sub,right=species_sub, how='outer', \n",
    "                       left_on='species', right_on='species_id')\n",
    "merged_outer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automating data processing using For Loops\n",
    "\n",
    "So far, we've used Python and the pandas library to explore and manipulate\n",
    "individual datasets by hand, much like we would do in a spreadsheet. The beauty\n",
    "of using a programming language like Python, though, comes from the ability to\n",
    "automate data processing through the use of loops and functions.\n",
    "\n",
    "## For loops\n",
    "\n",
    "Loops allow us to repeat a workflow (or series of actions) a given number of\n",
    "times or while some condition is true. We would use a loop to automatically\n",
    "process data that's stored in multiple files (daily values with one file per\n",
    "year, for example). Loops lighten our work load by performing repeated tasks\n",
    "without our direct involvement and make it less likely that we'll introduce\n",
    "errors by making mistakes while processing each file by hand.\n",
    "\n",
    "Let's write a simple for loop that simulates what a kid might see during a\n",
    "visit to the zoo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "animals = ['lion','tiger','crocodile','vulture','hippo']\n",
    "print(animals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for creatures in animals:\n",
    "    print(creatures)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line defining the loop must start with `for` and end with a colon, and the\n",
    "body of the loop must be indented.\n",
    "\n",
    "In this example, `creature` is the loop variable that takes the value of the next\n",
    "entry in `animals` every time the loop goes around. We can call the loop variable\n",
    "anything we like. After the loop finishes, the loop variable will still exist\n",
    "and will have the value of the last entry in the collection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for creatures in animals:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "creatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are not asking python to print the value of the loop variable anymore, but\n",
    "the for loop still runs and the value of `creature` changes on each pass through\n",
    "the loop. The statement `pass` in the body of the loop just means \"do nothing\".\n",
    "\n",
    "### Challenge:\n",
    "\n",
    "1. What happens if we don't include the `pass` statement?\n",
    "\n",
    "2. Rewrite the loop so that the animals are separated by commas, not new lines\n",
    "(Hint: You can concatenate strings using a plus sign. For example,\n",
    "`print(string1 + string2)` outputs 'string1string2')."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "animals = ['lion', 'tiger', 'crocodile', 'vulture', 'hippo']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test = \"the animals are\"\n",
    "for creatures in animals:\n",
    "    test = test + \" ,\" + creatures\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file we've been using so far, `surveys.csv`, contains 25 years of data and is\n",
    "very large. We would like to separate the data for each year into a separate\n",
    "file.\n",
    "\n",
    "Let's start by making a new directory inside the folder `data` to store all of\n",
    "these files using the module `os`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.mkdir(\"yearly_files/test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command `os.mkdir` is equivalent to `mkdir` in the shell. Just so we are\n",
    "sure, we can check that the new directory was created within the `data` folder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir('yearly_files/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The command `os.listdir` is equivalent to `ls` in the shell.\n",
    "\n",
    "Previously, we saw how to use the library pandas to load the species\n",
    "data into memory as a DataFrame, how to select a subset of the data using some\n",
    "criteria, and how to write the DataFrame into a csv file. Let's write a script\n",
    "that performs those three steps in sequence for the year 2002:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Load the data into a DataFrame\n",
    "surveys_df = pd.read_csv('surveys.csv')\n",
    "\n",
    "# Select only data for 2002\n",
    "surveys2002 = surveys_df[surveys_df.year == 2002]\n",
    "\n",
    "# Write the new DataFrame to a csv file\n",
    "surveys2002.to_csv('data/yearly_files/surveys2002.csv')\n",
    "```\n",
    "\n",
    "To create yearly data files, we could repeat the last two commands over and\n",
    "over, once for each year of data. Repeating code is neither elegant nor\n",
    "practical, and is very likely to introduce errors into your code. We want to\n",
    "turn what we've just written into a loop that repeats the last two commands for\n",
    "every year in the dataset.\n",
    "\n",
    "Let's start by writing a loop that simply prints the names of the files we want\n",
    "to create - the dataset we are using covers 1977 through 2002, and we'll create\n",
    "a separate file for each of those years. Listing the filenames is a good way to\n",
    "confirm that the loop is behaving as we expect.\n",
    "\n",
    "We have seen that we can loop over a list of items, so we need a list of years \n",
    "to loop over. We can get the years in our DataFrame with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df['year'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "but we want only unique years, which we can get using the `unique` function \n",
    "which we have already seen.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_df['year'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting this into our for loop we get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for year in surveys_df['year'].unique():\n",
    "    filename = 'yearly_files/surveys' + str(year) + '.csv'\n",
    "    print(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now add the rest of the steps we need to create separate text files.\n",
    "Once finished look inside the `yearly_files` directory and check a couple of the files you\n",
    "just created to confirm that everything worked as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for year in surveys_df['year'].unique():\n",
    "    #creating filename\n",
    "    filename = 'yearly_files/surveys' + str(year) + '.csv'\n",
    "    # extracting data of a specific year\n",
    "    surveys_year = surveys_df[surveys_df['year']==year]\n",
    "    # writing to file\n",
    "    surveys_year.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir('yearly_files/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writing Unique FileNames\n",
    "\n",
    "Notice that the code above created a unique filename for each year.\n",
    "\n",
    "\tfilename = 'yearly_files/surveys_year' + str(year) + '.csv'\n",
    "\n",
    "Let's break down the parts of this name:\n",
    "\n",
    "* The first part is simply some text that specifies the directory to store our\n",
    "  data file in (data/yearly_files/) and the first part of the file name\n",
    "  (surveys): `'yearly_files/surveys_year'`\n",
    "* We can concatenate this with the value of a variable, in this case `year` by\n",
    "  using the plus `+` sign and the variable we want to add to the file name: `+\n",
    "  str(year)`\n",
    "* Then we add the file extension as another text string: `+ '.csv'`\n",
    "\n",
    "Notice that we use single quotes to add text strings. The variable is not\n",
    "surrounded by quotes. This code produces the string\n",
    "`yearly_files/surveys_year2002.csv` which contains the path to the new filename\n",
    "AND the file name itself.\n",
    "\n",
    "### Challenge\n",
    "\n",
    "1. Some of the surveys you saved are missing weight data (they have null values that\n",
    "show up as NaN - Not A Number - in the DataFrames and do not show up in the text\n",
    "files). Modify the for loop so that the entries with null values are not\n",
    "included in the yearly files.\n",
    "\n",
    "2. What happens if there is no data for a year in the sequence (for example,\n",
    "imagine we had used 1976 as the start year in `range`)?\n",
    "\n",
    "3. Let's say you only want to look at data from a given multiple of years. How would you modify your loop in order to generate a data file for only every 5th year, starting from 1977?\n",
    "\n",
    "4. Instead of splitting out the data by years, a colleague wants to do analyses each species separately. How would you write a unique csv file for each species?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for year in surveys_df['year'].unique():\n",
    "    #creating filename\n",
    "    filename = 'yearly_files/surveys' + str(year) + '.csv'\n",
    "    # extracting data of a specific year\n",
    "    surveys_year = surveys_df[(surveys_df['year']==year) & (surveys_df['wgt']>0)]\n",
    "    # writing to file\n",
    "    surveys_year.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for year in surveys_df['year'].unique():\n",
    "    #creating filename\n",
    "    filename = 'yearly_files/surveys_noNAwgt_' + str(year) + '.csv'\n",
    "    # extracting data of a specific year\n",
    "    surveys_year = surveys_df[(surveys_df['year']==year) & (~surveys_df['wgt'].isnull())]\n",
    "    # writing to file\n",
    "    surveys_year.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "surveys_year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building reusable and modular code with functions\n",
    "\n",
    "Suppose that separating large data files into individual yearly files is a task\n",
    "that we frequently have to perform. We could write a **for loop** like the one above\n",
    "every time we needed to do it but that would be time consuming and error prone.\n",
    "A more elegant solution would be to create a reusable tool that performs this\n",
    "task with minimum input from the user. To do this, we are going to turn the code\n",
    "we've already written into a function.\n",
    "\n",
    "Functions are reusable, self-contained pieces of code that are called with a\n",
    "single command. They can be designed to accept arguments as input and return\n",
    "values, but they don't need to do either. Variables declared inside functions\n",
    "only exist while the function is running and if a variable within the function\n",
    "(a local variable) has the same name as a variable somewhere else in the code,\n",
    "the local variable hides but doesn't overwrite the other.\n",
    "\n",
    "Every method used in Python (for example, `print`) is a function, and the\n",
    "libraries we import (say, `pandas`) are a collection of functions. We will only\n",
    "use functions that are housed within the same code that uses them, but it's also\n",
    "easy to write functions that can be used by different programs.\n",
    "\n",
    "Functions are declared following this general structure:\n",
    "\n",
    "```python\n",
    "def this_is_the_function_name(input_argument1, input_argument2):\n",
    "\n",
    "    # The body of the function is indented\n",
    "    # This function prints the two arguments to screen\n",
    "    print('The function arguments are:', input_argument1, input_argument2, \n",
    "    '(this is done inside the function!)')\n",
    "\n",
    "    # And returns their product\n",
    "    return input_argument1 * input_argument2\n",
    "```\n",
    "\n",
    "The function declaration starts with the word `def`, followed by the function\n",
    "name and any arguments in parenthesis, and ends in a colon. The body of the\n",
    "function is indented just like loops are. If the function returns something when\n",
    "it is called, it includes a return statement at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's define this function\n",
    "def this_is_the_function_name(input_argument1, input_argument2):\n",
    "    \"\"\"\n",
    "    input_argument1: a float or numerical type\n",
    "    \"\"\"\n",
    "    # The body of the function is indented\n",
    "    # This function prints the two arguments to screen\n",
    "    print('The function arguments are:', input_argument1, input_argument2, '(this is done inside the function!)')\n",
    "\n",
    "    # And returns their product\n",
    "    return input_argument1 * input_argument2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#and now let's call the function:\n",
    "result = this_is_the_function_name(4,4)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "\n",
    "1. Change the values of the arguments in the function and check its output\n",
    "2. Try calling the function by giving it the wrong number of arguments (not 2)\n",
    "   or not assigning the function call to a variable (no `product_of_inputs =`)\n",
    "3. Declare a variable inside the function and test to see where it exists (Hint:\n",
    "   can you print it from outside the function?)\n",
    "4. Explore what happens when a variable both inside and outside the function\n",
    "   have the same name. What happens to the global variable when you change the\n",
    "   value of the local variable?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "this_is_the_function_name(1, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def this_other_function(in1, in2, in3=74646):\n",
    "    new_variable = 3\n",
    "    print(new_variable, in1, in2)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "this_other_function(1,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now turn our code for saving yearly data files into a function. There are\n",
    "many different \"chunks\" of this code that we can turn into functions, and we can\n",
    "even create functions that call other functions inside them. Let's first write a\n",
    "function that separates data for just one year and saves that data to a file:\n",
    "\n",
    "```python\n",
    "def one_year_csv_writer(this_year, all_data):\n",
    "    \"\"\"\n",
    "    Writes a csv file for data from a given year.\n",
    "\n",
    "    this_year --- year for which data is extracted\n",
    "    all_data --- DataFrame with multi-year data\n",
    "    \"\"\"\n",
    "\n",
    "    # Select data for the year\n",
    "    surveys_year = all_data[all_data.year == this_year]\n",
    "\n",
    "    # Write the new DataFrame to a csv file\n",
    "    filename = 'yearly_files/function_surveys_year' + str(this_year) + '.csv'\n",
    "    surveys_year.to_csv(filename)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_year_csv_writer(this_year, all_data):\n",
    "    \"\"\"\n",
    "    Writes a csv file for data from a given year.\n",
    "\n",
    "    this_year --- year for which data is extracted\n",
    "    all_data --- DataFrame with multi-year data\n",
    "    \"\"\"\n",
    "\n",
    "    # Select data for the year\n",
    "    surveys_year = all_data[all_data.year == this_year]\n",
    "\n",
    "    # Write the new DataFrame to a csv file\n",
    "    filename = 'yearly_files/function_surveys_year' + str(this_year) + '.csv'\n",
    "    surveys_year.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for year in surveys_df['year'].unique():\n",
    "    one_year_csv_writer(year, surveys_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text between the two sets of triple double quotes is called a docstring and\n",
    "contains the documentation for the function. It does nothing when the function\n",
    "is running and is therefore not necessary, but it is good practice to include\n",
    "docstrings as a reminder of what the code does. Docstrings in functions also\n",
    "become part of their 'official' documentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_year_csv_writer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_year_csv_writer(2002,surveys_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We changed the root of the name of the csv file so we can distinguish it from\n",
    "the one we wrote before. Check the `yearly_files` directory for the file. Did it\n",
    "do what you expect?\n",
    "\n",
    "What we really want to do, though, is create files for multiple years without\n",
    "having to request them one by one. Let's write another function that replaces\n",
    "the entire For loop by simply looping through a sequence of years and repeatedly\n",
    "calling the function we just wrote, `one_year_csv_writer`:\n",
    "\n",
    "\n",
    "```python\n",
    "def yearly_data_csv_writer(start_year, end_year, all_data):\n",
    "    \"\"\"\n",
    "    Writes separate csv files for each year of data.\n",
    "\n",
    "    start_year --- the first year of data we want\n",
    "    end_year --- the last year of data we want\n",
    "    all_data --- DataFrame with multi-year data\n",
    "    \"\"\"\n",
    "\n",
    "    # \"end_year\" is the last year of data we want to pull, so we loop to end_year+1\n",
    "    for year in range(start_year, end_year+1):\n",
    "        one_year_csv_writer(year, all_data)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "range(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because people will naturally expect that the end year for the files is the last\n",
    "year with data, the for loop inside the function ends at `end_year + 1`. \n",
    "This is because when we specify `range()` the last number is not included, try it for yourself.\n",
    "\n",
    "By writing the entire loop into a function, we've made a reusable tool for whenever\n",
    "we need to break a large data file into yearly files. Because we can specify the\n",
    "first and last year for which we want files, we can even use this function to\n",
    "create files for a subset of the years available. This is how we call this\n",
    "function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BEWARE!** If you are using IPython Notebooks and you modify a function, you MUST\n",
    "re-run that cell in order for the changed function to be available to the rest\n",
    "of the code. Nothing will visibly happen when you do this, though, because\n",
    "simply defining a function without *calling* it doesn't produce an output. Any\n",
    "cells that use the now-changed functions will also have to be re-run for their\n",
    "output to change.\n",
    "\n",
    "### Challenge:\n",
    "\n",
    "1. Add two arguments to the functions we wrote that take the path of the\n",
    "   directory where the files will be written and the root of the file name.\n",
    "   Create a new set of files with a different name in a different directory.\n",
    "2. How could you use the function `yearly_data_csv_writer` to create a csv file\n",
    "   for only one year? (Hint: think about the syntax for `range`)\n",
    "3. Make the functions return a list of the files they have written. There are\n",
    "   many ways you can do this (and you should try them all!): either of the\n",
    "   functions can print to screen, either can use a return statement to give back\n",
    "   numbers or strings to their function call, or you can use some combination of\n",
    "   the two. You could also try using the `os` library to list the contents of\n",
    "   directories.\n",
    "4. (If we have time in class, otherwise have a looka t home)\n",
    "   Explore what happens when variables are declared inside each of the functions\n",
    "   versus in the main (non-indented) body of your code. What is the scope of the\n",
    "   variables (where are they visible)? What happens when they have the same name\n",
    "   but are given different values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_year_csv_writer(this_year, all_data, directory, file_root):\n",
    "    \"\"\"\n",
    "    Writes a csv file for data from a given year.\n",
    "\n",
    "    this_year --- year for which data is extracted\n",
    "    all_data --- DataFrame with multi-year data\n",
    "    \"\"\"\n",
    "\n",
    "    # Select data for the year\n",
    "    surveys_year = all_data[all_data.year == this_year]\n",
    "\n",
    "    # Write the new DataFrame to a csv file\n",
    "    filename = 'yearly_files/function_surveys_year' + str(this_year) + '.csv'\n",
    "    surveys_year.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_year_csv_writer(this_year, all_data, root_dir='yearly_files/',\n",
    "                        file_name='functions_surveys_year'):\n",
    "    \"\"\"\n",
    "    Writes a csv file for data from a given year.\n",
    "\n",
    "    this_year --- year for which data is extracted\n",
    "    all_data --- DataFrame with multi-year data\n",
    "    root_dir --- The root directory: must end with a /\n",
    "    file_name ---- The name of the file to be called\n",
    "    \"\"\"\n",
    "\n",
    "    # Select data for the year\n",
    "    surveys_year = all_data[all_data.year == this_year]\n",
    "\n",
    "    # Write the new DataFrame to a csv file\n",
    "    filename = root_dir + file_name + str(this_year) + '.csv'\n",
    "    surveys_year.to_csv(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_year_csv_writer(2001, surveys_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "os.listdir('yearly_files/test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The functions we wrote demand that we give them a value for every argument.\n",
    "Ideally, we would like these functions to be as flexible and independent as\n",
    "possible. Let's modify the function `yearly_data_csv_writer` so that the\n",
    "`start_year` and `end_year` default to the full range of the data if they are\n",
    "not supplied by the user. Arguments can be given default values with an equal\n",
    "sign in the function declaration. Any arguments in the function without default\n",
    "values (here, `all_data`) is a required argument and MUST come before the\n",
    "argument with default values (which are optional in the function call).\n",
    "\n",
    "```python\n",
    "    def yearly_data_arg_test(all_data, start_year = 1977, end_year = 2002):\n",
    "        \"\"\"\n",
    "        Modified from yearly_data_csv_writer to test default argument values!\n",
    "\n",
    "        start_year --- the first year of data we want --- default: 1977\n",
    "        end_year --- the last year of data we want --- default: 2002\n",
    "        all_data --- DataFrame with multi-year data\n",
    "        \"\"\"\n",
    "\n",
    "        return start_year, end_year\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test function\n",
    "# using \\t in the `print` statement inserts tabs, used to align the text and make it easier to read."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if our dataset doesn't start in 1977 and end in 2002? We can modify the\n",
    "function so that it looks for the start and end years in the dataset if those\n",
    "dates are not provided:\n",
    "\n",
    "```python\n",
    "    def yearly_data_arg_test(all_data, start_year = None, end_year = None):\n",
    "        \"\"\"\n",
    "        Modified from yearly_data_csv_writer to test default argument values!\n",
    "\n",
    "        start_year --- the first year of data we want --- default: None - check all_data\n",
    "        end_year --- the last year of data we want --- default: None - check all_data\n",
    "        all_data --- DataFrame with multi-year data\n",
    "        \"\"\"\n",
    "\n",
    "        if not start_year:\n",
    "            start_year = min(all_data.year)\n",
    "        if not end_year:\n",
    "            end_year = max(all_data.year)\n",
    "\n",
    "        return start_year, end_year\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "        if start_year is None:\n",
    "            start_year = min(all_data.year)\n",
    "        if end_year is None:\n",
    "            end_year = max(all_data.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default values of the `start_year` and `end_year` arguments in the function\n",
    "`yearly_data_arg_test` are now `None`. This is a build-it constant in Python\n",
    "that indicates the absence of a value - essentially, that the variable exists in\n",
    "the namespace of the function (the directory of variable names) but that it\n",
    "doesn't correspond to any existing object.\n",
    "\n",
    "### Challenge:\n",
    "\n",
    "1. What type of object corresponds to a variable declared as `None`? (Hint:\n",
    "create a variable set to `None` and use the function `type()`)\n",
    "\n",
    "2. Compare the behavior of the function `yearly_data_arg_test` when the\n",
    "arguments have `None` as a default and when they do not have default values.\n",
    "\n",
    "3. What happens if you only include a value for `start_year` in the function\n",
    "call? Can you write the function call with only a value for `end_year`? (Hint:\n",
    "think about how the function must be assigning values to each of the arguments -\n",
    "this is related to the need to put the arguments without default values before\n",
    "those with default values in the function definition!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The body of the test function now has two conditional 'loops' (if statement) that\n",
    "check the values of `start_year` and `end_year`. If statements execute the body of\n",
    "the 'loop' when some condition is met. They commonly look something like this:\n",
    "\n",
    "```python\n",
    "    a = 5\n",
    "\n",
    "    if a<0: # meets first condition?\n",
    "\n",
    "        # if a IS less than zero\n",
    "        print('a is a negative number')\n",
    "\n",
    "    elif a>0: # did not meet first condition. meets second condition?\n",
    "\n",
    "        # if a ISN'T less than zero and IS more than zero\n",
    "        print('a is a positive number')\n",
    "\n",
    "    else: # met neither condition\n",
    "\n",
    "        # if a ISN'T less than zero and ISN'T more than zero\n",
    "        print('a must be zero!')\n",
    "\n",
    "    a is a positive number\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the value of `a` to see how this function works. The statement `elif`\n",
    "means \"else if\", and all of the conditional statements must end in a colon.\n",
    "\n",
    "The if statements in the function `yearly_data_arg_test` check whether there is an\n",
    "object associated with the variable names `start_year` and `end_year`. If those\n",
    "variables are `None`, the if statements return the boolean `True` and execute whatever\n",
    "is in their body. On the other hand, if the variable names are associated with\n",
    "some value (they got a number in the function call), the if statements return `False`\n",
    "and do not execute. The opposite conditional statements, which would return\n",
    "`True` if the variables were associated with objects (if they had received value\n",
    "in the function call), would be `if start_year` and `if end_year`.\n",
    "\n",
    "As we've written it so far, the function `yearly_data_arg_test` associates\n",
    "values in the function call with arguments in the function definition just based\n",
    "in their order. If the function gets only two values in the function call, the\n",
    "first one will be associated with `all_data` and the second with `start_year`,\n",
    "regardless of what we intended them to be. We can get around this problem by\n",
    "calling the function using keyword arguments, where each of the arguments in the\n",
    "function definition is associated with a keyword and the function call passes\n",
    "values to the function using these keywords:\n",
    "\n",
    "```python\n",
    "    def yearly_data_arg_test(all_data, start_year = None, end_year = None):\n",
    "        \"\"\"\n",
    "        Modified from yearly_data_csv_writer to test default argument values!\n",
    "\n",
    "        start_year --- the first year of data we want --- default: None - check all_data\n",
    "        end_year --- the last year of data we want --- default: None - check all_data\n",
    "        all_data --- DataFrame with multi-year data\n",
    "        \"\"\"\n",
    "\n",
    "        if not start_year:\n",
    "            start_year = min(all_data.year)\n",
    "        if not end_year:\n",
    "            end_year = max(all_data.year)\n",
    "\n",
    "        return start_year, end_year\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#define function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#test function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge:\n",
    "\n",
    "1. Rewrite the `one_year_csv_writer` and `yearly_data_csv_writer` functions to\n",
    "have keyword arguments with default values\n",
    "\n",
    "2. Modify the functions so that they don't create yearly files if there is no\n",
    "data for a given year and display an alert to the user (Hint: use conditional\n",
    "statements and if loops to do this. For an extra challenge, use `try`\n",
    "statements!)\n",
    "\n",
    "3. The code below checks to see whether a directory exists and creates one if it\n",
    "doesn't. Add some code to your function that writes out the CSV files, to check\n",
    "for a directory to write to.\n",
    "\n",
    "```Python\n",
    "\tif 'dir_name_here' in os.listdir('.'):\n",
    "\t    print('Processed directory exists')\n",
    "\telse:\n",
    "\t    os.mkdir('dir_name_here')\n",
    "\t    print('Processed directory created')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. The code that you have written so far to loop through the years is good,\n",
    "however it is not necessarily reproducible with different datasets.\n",
    "For instance, what happens to the code if we have additional years of data\n",
    "in our CSV files? Using the tools that you learned in the previous activities,\n",
    "make a list of all years represented in the data. Then create a loop to process\n",
    "your data, that begins at the earliest year and ends at the latest year using\n",
    "that list.\n",
    "\n",
    "HINT: you can create a loop with a list as follows: `for years in year_list:`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
